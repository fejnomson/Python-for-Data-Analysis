# Combining and merging dat sets
pandas.merge # join rows based on keys


# db style df merges
df1 = DataFrame({
	'key': ['b', 'b', 'a', 'c', 'a', 'a', 'b'],
	'data1': range(7)
})
df2 = DataFrame({
	'key': ['a', 'b', 'd'],
	'data2': range(3)
})
df1
df2
pd.merge(df1, df2) # many to one; but drops if not in both tables
pd.merge(df1, df2, on = 'key')
df3 = DataFrame({
	'lkey': ['b', 'b', 'a', 'c', 'a', 'a', 'b'],
	'data1': range(7)
})
df4 = DataFrame({
	'rkey': ['a', 'b', 'd'],
	'data2': range(3)
})
pd.merge(df3, df4, left_on = 'lkey', right_on = 'rkey')
# inner join: output keys are intersection of input keys
# outer join: union of the two keys
pd.merge(df1, df2, how = 'outer')

df1 = DataFrame({
	'key': ['b', 'b', 'a', 'c', 'a', 'b'],
	'data1': range(6)
})
df2 = DataFrame({
	'key': ['a', 'b', 'a', 'b', 'd'],
	'data2': range(5)
})
pd.merge(df1, df2, on = 'key', how = 'left')
pd.merge(df1, df2, how = 'inner')
left = DataFrame({
	'key1': ['foo', 'foo', 'bar'],
	'key2': ['one', 'two', 'one'],
	'rval': [1, 2, 3]
})
right = DataFrame({
	'key1': ['foo', 'foo', 'bar', 'bar'],
	'key2': ['one', 'one', 'one', 'two'],
	'rval': [4, 5, 6, 7]
})
pd.merge(left, right, on = ['key1', 'key2'], how = 'outer')
pd.merge(
	left, right, on = 'key1', suffixes = ('_left', '_right')
)
# not sure if i've used this before, but LOVE having control over the
# 	suffixes
# Wondering when and why you'd turn 'copy' off / why it always copies by
# 	default.


# Merging on Index
left1 = DataFrame({
	'key': ['a', 'b', 'a', 'a', 'b', 'c'],
	'value': range(6)
})
right1 = DataFrame({'group_val': [3.5, 7]},	index = ['a', 'b'])
pd.merge(left1, right1, left_on = 'key', right_index = True)
pd.merge(left1, right1, left_on = 'key', right_index = True, how = 'outer')
lefth = DataFrame({
	'key1': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada'],
	'key2': [2000, 2001, 2002, 2001, 2002],
	'data': np.arange(5.)
})
righth = DataFrame(
	np.arange(12).reshape((6, 2)),
	index = [
		['Nevada', 'Nevada', 'Ohio', 'Ohio', 'Ohio', 'Ohio'],
		[2001, 2000, 2000, 2000, 2001, 2002]
	],
	columns = ['event1', 'event2']
)
pd.merge(lefth, righth, left_on = ['key1', 'key2'], right_index = True)
pd.merge(
	lefth,
	righth,
	left_on = ['key1', 'key2'],
	right_index = True,
	how = 'outer'
)
left2 = DataFrame(
	[[1., 2.], [3., 4.], [5., 6.]],
	index = ['a', 'c', 'e'],
	columns = ['Ohio', 'Nevada']
)
right2 = DataFrame(
	[[7., 8.], [9., 10.], [11., 12.], [13, 14]],
	index = ['b', 'c', 'd', 'e'],
	columns = ['Missouri', 'Alabama']
)
pd.merge(left2, right2, how = 'outer', left_index = True, right_index = True)
# JOIN() METHOD IS INTENDED FOR MERGING BY INDEX
left2.join(right2, how = 'outer') # can also use to join list of DFs together by index
left1.join(right1, on = 'key') # can do ix one one, columns on another
another = DataFrame(
	[[7., 8.], [9., 10.], [11., 12.], [16., 17.]],
	index = ['a', 'c', 'e', 'f'],
	columns = ['New York', 'Oregon']
)
left2.join([right2, another]) # alternative to pd.concat(), not sure why you'd do one or the other though
left2.join([right2, another], how = 'outer')


# Concatenating Along an Axis
sl = Series([0, 1], index = ['a', 'b'])
s2 = Series([2, 3, 4], index = ['c', 'd', 'e'])
s3 = Series([5, 6], index = ['f', 'g'])
pd.concat([sl, s2, s3])
pd.concat([sl, s2, s3], axis = 1)
s4 = pd.concat([sl*5, s3])
pd.concat([sl, s4], axis = 1)
# by column, union of keys
pd.concat([sl, s4], axis = 1, join = 'inner')
# by column, intersect of keys
pd.concat([sl, s4], axis = 1, join_axes = [['a', 'c', 'b', 'e']])
# specify which keys to join on
result = pd.concat([sl, sl, s3], keys = ['one', 'two', 'three'])
result
# So you can keep track of which obs. came from which chunk
result.unstack()
pd.concat([sl, s2, s3], axis = 1, keys = ['one', 'two', 'three'])
# keys become col names if you go by cols
df1 = DataFrame(np.arange(6).reshape(3, 2), index = ['a', 'b', 'c'], columns = ['one', 'two'])
df2 = DataFrame(5 + np.arange(4).reshape(2, 2), index = ['a', 'c'], columns = ['three', 'four'])
pd.concat([df1, df2], axis = 1, keys = ['level1', 'level2'])
# looks like it stacks the columns automatically, then throws the 'key' / level on top of each group of columns.
pd.concat({'level1': df1, 'level2': df2}, axis = 1)
# if pass dict, dicts keys are automatically used as keys
pd.concat([df1, df2], axis = 1, keys = ['level1', 'level2'], names = ['upper', 'lower'])
# name the keys you pass that become an index



df1 = DataFrame(
	np.random.randn(3, 4),
	columns = ['a', 'b', 'c', 'd']
)
df2 = DataFrame(
	np.random.randn(2, 3),
	columns = ['b', 'd', 'a']
)
df1
df2
pd.concat([df1, df2], ignore_index = True)
# this is more R-like: basically ignore indexes / keys

pd.concat([df1, df2], join = 'inner')
pd.concat([df1, df2], join = 'inner', ignore_index = True)
# basically plyr::rbind.fill
pd.concat([df1, df2], axis = 1, join = 'inner')
# so weird that you have a clean and fast rbind.fill and cbind.fill
# 	with a bunch of clean arguments in pandas, but not really any good
# 	equivalents in R



# Combining Data with Overlap -------------------------------------------------
a = Series(
	[np.nan, 2.5, np.nan, 3.5, 4.5, np.nan],
	index = ['f', 'e', 'd', 'c', 'b', 'a']
)
b = Series(
	np.arange(len(a), dtype = np.float64),
	index = ['f', 'e', 'd', 'c', 'b', 'a']
)
b[-1] = np.nan
np.where(pd.isnull(a), b, a)
b[:-2].combine_first(a[2:])
# so this takes the first at each ix. so if there's nothing in a, but in b;
# 	if there's nothing in b, put in a, if both populated, put in the first one.
df1 = DataFrame({
	'a': [1., np.nan, 5., np.nan],
	'b': [np.nan, 2., np.nan, 6.],
	'c': range(2, 18, 4)
})
df2 = DataFrame({
	'a': [5., 4., np.nan, 3., 7.],
	'b': [np.nan, 3., 4., 6., 8.]
})
df1.combine_first(df2)
# this is series.combine_first() but by column for each data frame. Basically,
# 	if anything is 'missing' from a column in the first data frame, replace
# 	it with whatever's in the second column.
# kind of like ifelse, but incorporates key-based alignment, as opposed to 
# 	relying strictly on integer-based alignment
# Combining Data with Overlap -------------------------------------------------



# Reshaping and Pivoting ------------------------------------------------------
# Reshaping with Hierarchical Indexing
# two main functions are stack() and unstack()
# stack(): columns to rows
# unstack(): rows to columns
data = DataFrame(
	np.arange(6).reshape((2, 3)),
	index = pd.Index(['Ohio', 'Colorado'], name = 'state'),
	columns = pd.Index(['one', 'two', 'three'], name = 'number')
)
data
result = data.stack()
result.unstack() # stack and unstack default to inner-most index level
result.unstack(0) # manually specify NOT using inner-most index level
result.unstack('state') # specify level to unstack by level name

s1 = Series([0, 1, 2, 3], index = ['a', 'b', 'c', 'd'])
s2 = Series([4, 5, 6], index = ['c', 'd', 'e'])
data2 = pd.concat([s1, s2], keys = ['one', 'two'])
# 'a' & 'b' is missing from 'two', so there has to be a slot, but it'll be NA
data2.unstack()
data2.unstack().stack() # drops missing by default, so lose the NAs introduced
data2.unstack().stack(dropna = False)

df = DataFrame(
	{'left': result, 'right': result + 5},
	columns = pd.Index(['left', 'right'], name = 'side')
)
df.unstack('state')
df.unstack('state').stack('side')
# very similar to charting I do in Tableau


# Pivoting 'long' to 'wide' format
# pivot is similar to tidyr, pd.stack and pd.unstack: for going back and 
# 	forth between long format.
# Long format is used in databases, where you'd assign two columns as
# 	keys. I've used this format for plotting, e.g. gantt plotting. But
# 	long format isn't that useful for analysis...
# pivot is just a convenience, it's the same as
# 	df.set_index(['key1', 'key2']).unstack('item')
# Reshaping and Pivoting ------------------------------------------------------



# Data Transformation ---------------------------------------------------------
# removing duplicates
data = DataFrame({
	'k1': ['one'] * 3 + ['two'] * 4,
	'k2': [1, 1, 2, 3, 3, 4, 4]
})
data
data.duplicated() # same as r duplicated(df)
data.drop_duplicates() # drops dups
data['v1'] = range(7)
data.drop_duplicates(['k1'])
# this is just so much cleaner than R: df[!duplicated(df[c('k1')])]
data.drop_duplicates(['k1', 'k2'], take_last = True)


# Transforming Data Using a Fuction or Maping

data = DataFrame({
	'food': ['bacon', 'pulled pork', 'bacon', 'Pastrami', 'corned beef', 'Bacon', 'pastrami', 'honey ham', 'nova lox'],
	'ounces': [4, 3, 12, 6, 7.5, 8, 3, 5, 6]
})
meat_to_animal = {
	'bacon': 'pig',
	'pulled pork': 'pig',
	'pastrami': 'cow',
	'corned beef': 'cow',
	'honey ham': 'pig',
	'nova lox': 'salmon'
}
data['animal'] = data['food'].map(str.lower).map(meat_to_animal)
data['food'].map(lambda x: meat_to_animal[x.lower()]) # so for every element, to lower case, then subset the meat_to_animal dict based on lower case
# crazy that .map can take a dict of old and new values
x = Series(['chicago', 'madison', 'seattle', 'los angeles', 'new york', 'DC'])
x.map({
	'chicago': 'midwest',
	'madison': 'midwest',
	'seattle': 'west',
	'los angeles': 'west',
	'new york': 'east',
	'DC': 'east'
})
# so this can be lapply-like OR plyr::map_values-like


# Replacing Values
data = Series([1., -999., 2., -999., -1000., 3.])
data
data.replace(-999, np.nan)
# MUCH BETTER FOR plyr::map_values like applicaitons
data.replace([-999, -1000], np.nan)
data.replace({-999: np.nan, -1000: 0})


# Renaming Axis Indexes
data = DataFrame(
	np.arange(12).reshape((3, 4)),
	index = ['Ohio', 'Colorado', 'New York'],
	columns = ['one', 'two', 'three', 'four']
)
# map method for indexes
data.index.map(str.upper) # doesn't work...
data.index.map(lambda x: x.upper()) # works
data.rename(index = str.title, columns = str.upper)
# transforms without modifying the original. Looks like this applies a function to each element??
# 	like it actualy map()s row and column ixs
data.rename(index = {'Ohio': 'INDIANA'}, columns = {'three': 'peekaboo'})
# {current: new}
data.rename(index = {'Ohio': 'INDIANA'}, inplace = True)


# Discretization and Binning
ages = [20, 22, 25, 27, 21, 23, 37, 31, 61, 45, 41, 32]
bins = [18, 25, 35, 60, 100]
cats = pd.cut(ages, bins)
# returns Categorical object
# like array of string indicating bin name
cats.codes # looks like this returns the bin number, as in findInterval()
cats.categories
pd.value_counts(cats)
pd.cut(ages, [18, 26, 36, 61, 100], right = False)
# pass own bin names
group_names = ['Youth', 'YoungAdult', 'MiddleAged', 'Senior']
pd.cut(ages, bins, labels = group_names) # pass own bin labels
# so maybe the labels could be dates
data = np.random.rand(20)
pd.cut(data, 4, precision = 2) # specify the number of bins instead of the bin edges
data = np.random.randn(1000)
cats = pd.qcut(data, 4) # cut into 4 equally sized groups
pd.qcut(data, [0, 0.1, 0.5, 0.9, 1.]) # pass own quantiles
cats.get_values()
# LOOKS LIKE THIS RETURNS THE BIN FOR EACH VALUE.
# so you could do something like:
# DataFrame({
#   'Out.Punch': out_punch
# 	'Index.Date': pd.cut(out_punch, dates).get_values()
# })

# Detecting and Filtering Outliers
np.random.seed(12345)
data = DataFrame(np.random.randn(1000, 4))
data.describe()
col = data[3]
col[np.abs(col) > 3]
# filter to rows that have a value < -3 or > 3
data[(np.abs(data) > 3).any(1)] # df.any(): any condition true over requested axis
# cap values outside of 3
data[np.abs(data) > 3] = np.sign(data) * 3


# Permutation and Random Sampling
# change order of df or series, like vect[sample(length(vect))]
df = DataFrame(np.arange(5* 4).reshape(5, 4))
sampler = np.random.permutation(5)
sampler
df.take(sampler)
# a bit different from r in that you can just do df.take(vect), instead
# 	of df[vect, ]
len(df)
# interesting that this takes rows. I think R is intuitive in that df
# 	inherits from list, but i NEVER use ncol(), and ALWAYS use nrow()
# 	so think it makes sense...
df.take(np.random.permutation(len(df))[:3])
# quick way to get random sample df, no replacement
bag = np.array([5, 7, -1, 6, 4])
sampler = np.random.randint(0, len(bag), size = 10)
sampler
draws = bag.take(sampler)
draws
# so the random sample is greater than len(df), use replacement


# Computing Indicator/Dummy Variables
df = DataFrame({
	'key': ['b', 'b', 'a', 'c', 'a', 'b'],
	'data1': range(6)
})
pd.get_dummies(df['key'])
# makes matrix indicating whether values are present in Series or not
dummies = pd.get_dummies(df['key'], prefix = 'key')
df_with_dummy = df[['data1']].join(dummies)
mnames = ['movie_id', 'title', 'genres']
path = '//firm.seyfarth.com/DFS/CHIUsers/JMonson/My Documents/Programming/Python/pydata-book-master/ch02/movielens/movies.dat'
movies = pd.read_table(path, sep = '::', header = None, names = mnames)
movies[:10]
genre_iter = (set(x.split('|')) for x in movies.genres) # Basically like a list of sets
genres = sorted(set.union(*genre_iter)) # The * means to unpack all items
dummies = DataFrame(np.zeros((len(movies), len(genres))), columns = genres)
for i, gen in enumerate(movies.genres):
	dummies.ix[i, gen.split('|')] = 1
movies_windic = movies.join(dummies.add_prefix('Genre_'))
movies_windic.ix[0]

values = np.random.rand(10)
values
bins = [0, 0.2, 0.4, 0.6, 0.8, 1]
pd.get_dummies(pd.cut(values, bins))


# String Manipulation
val = 'a,b,  guido'
val.split(',')
pieces = [x.strip() for x in val.split(',')]
first, second, third = pieces
'::'.join(pieces)
'guido' in val
val.index(',')
val.find(':')
val.index(':')
val.count(',')
val.replace(',', '::') # copy on modify
val.replace(',', '')
# 	overall, strings are so clean and simply in python
import re
text = 'foo 	  bar\t baz  \tqux'
re.split('\s+', text) # not a guru on regex, but could use learn regex the hard way
regex = re.compile('\s+') # FOR RE-USING THE SAME REGEX PATTERN
regex.split(text) 				# call methods from the regex object. could be useful
													# Think more efficient than string.split(regex, text)
regex.findall(text) 			# Wish it were this clean in R
r'C:\x' 									# "Raw string literals"; could be useful for
													# 	directory munging? makes equivalent to 'C:\\x'
r'G:\Kindred (68914)\Collins (11)\Work\Call BATCH\Reporting BATCH'
text = """
Dave dave@google.com
Steve steve@gmail.com
Rob rob@gmail.com
Ryan ryan@yahoo.com
"""
pattern = r'[A-Z0-9._%+_]+@[A-Z0-9.-]+\.[A-Z]{2,4}'
regex = re.compile(pattern, flags = re.IGNORECASE)
regex.findall(text) 			# Cool
m = regex.search(text)		# Returns match object. Mostly useful for start
													# and stop of the match. only returns for first
													# match.
text[m.start():m.end()]
print regex.sub('REDACTED', text) # Lulz.

pattern = r'([A-Z0-9._%+-]+)@(A-Z0-9.-]+)\.([A-Z]{2,4})'
regex = re.compile(pattern, flags = re.IGNORECASE)
m = regex.match('wesm@bright.net')
m.groups()								# Not sure why doesn't work...
regex.findall(text)
# I could spend a ton more time on this, but don't really think it's that
# 	relevant to what I'm doing right now. Will re-visit if necessary.


# Vectorized string functions in pandas
data = {
	'Dave': 'dave@google.com',
	'Steve': 'steve@gmail.com',
	'Rob': 'rob@gmail.com',
	'Wes': np.nan
}
data = Series(data)
# Can't use base string and regex methods, because they don't handle NAs;
# 	hence pandas string functionality
data.str.contains('gmail')
data.str.match('[A-Z]', flags = re.IGNORECASE)
matches = data.str.findall('[A-Z]', flags = re.IGNORECASE)
matches
matches.str.get(1) 					# for vectorized element retreival
matches.str[0] 							# alternate
matches.str[1:3]						# vectorised slicing
# So the point with pandas string methods is more vectorization of things
# 	that already exist in python, rather than duplicate methods or a 
#		stringr-like rewrite of the original for user friendlyness.


# Example: USDA Food Database
%bookmark pandas_book "//firm.seyfarth.com/DFS/CHIUsers/JMonson/My Documents/Programming/Python/pydata-book-master/"
%pwd
%cd pandas_book
%pwd
os.chdir("//firm.seyfarth.com/DFS/CHIUsers/JMonson/My Documents/Programming/Python/pydata-book-master/ch07")
import json
db = json.load(open('foods-2011-10-03.json'))
len(db)
db[0].keys()
db[0]['nutrients'][0]
nutrients = DataFrame(db[0]['nutrients'])
info_keys = ['description', 'group', 'id', 'manufacturer']
info = DataFrame(db, columns = info_keys)
info[:5]
pd.value_counts(info.group)[:10]
nutrients = []
for rec in db:
	fnuts = DataFrame(rec['nutrients'])
	fnuts['id'] = rec['id']
	nutrients.append(fnuts)
nutrients = pd.concat(nutrients, ignore_index = True)
nutrients.duplicated().sum()
nutrients = nutrients.drop_duplicates()
col_mapping = {'description': 'food', 'group':'fgroup'}
info = info.rename(columns = col_mapping, copy = False)
col_mapping = {'description': 'nutrient', 'group':'nutgroup'}
nutrients = nutrients.rename(columns = col_mapping, copy = False)
ndata = pd.merge(nutrients, info, on = 'id', how = 'outer')

result = ndata.groupby(['nutrient', 'fgroup'])['value'].quantile(0.5)
# result['Zinc', 'Zn'].order().plot(kind = 'barh')
result['Zinc, Zn'].sort_values().plot(kind = 'barh')

by_nutrient = ndata.groupby(['nutgroup', 'nutrient'])
get_maximum = lambda x: x.xs(x.value.idxmax())
get_minimum = lambda x: x.xs(x.value.idxmin())
max_foods = by_nutrient.apply(get_maximum)[['value', 'food']]
max_foods.food = max_foods.food.str[:50]
max_foods.ix['Amino Acids']['food']

